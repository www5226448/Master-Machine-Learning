{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "X,y=load_breast_cancer().data,load_breast_cancer().target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用sklearn求解Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9707602339181286"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as sklearn_LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "model=sklearn_LogisticRegression(penalty='none',fit_intercept=True,solver='lbfgs').fit(X_train,y_train)\n",
    "pre=model.predict(X_test)\n",
    "accuracy_score(y_test,pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "class LogisticRegression:\n",
    "    def __init__(self,learning_rate=0.004,max_iters=1800):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.max_iters=max_iters\n",
    "    def fit(self,X,y):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=np.mat(np.concatenate([X,X_],axis=1))       \n",
    "        y = np.mat(y).transpose()\n",
    "        m,n = X.shape\n",
    "        weights = np.mat(np.random.randn(n,1))\n",
    "        for k in range(self.max_iters):\n",
    "            h = expit(X*weights)  \n",
    "            error = h - y            \n",
    "            weights = weights - self.learning_rate * X.transpose()* error\n",
    "        self.coef_=np.array(weights).flatten()[:-1]\n",
    "        self.intercept_=weights[-1]\n",
    "        self.weights=weights\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)  \n",
    "        X=np.concatenate([X,X_],axis=1)\n",
    "        y_value=np.array(expit(X@self.weights)).flatten()                \n",
    "        y_pre=np.array([1 if i>0.5 else 0 for i in y_value])                 \n",
    "        return y_pre\n",
    "                    \n",
    "    def predict_proba(self,X):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=np.concatenate([X,X_],axis=1)   \n",
    "        y_value=np.array(expit(X@self.weights)).flatten()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9239766081871345\n"
     ]
    }
   ],
   "source": [
    "model=LogisticRegression().fit(X_train,y_train)\n",
    "y_pre=model.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用随机梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self,alpha=0.007,maxCycles=1800):\n",
    "        self.alpha=alpha\n",
    "        self.maxCycles=maxCycles\n",
    "    def fit(self,X, y, numIter=150):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=(np.concatenate([X,X_],axis=1)) \n",
    "        m,n = X.shape\n",
    "        weights = np.ones(n)   \n",
    "        for j in range(numIter):\n",
    "            dataIndex = list(range(m))\n",
    "            for i in range(m):\n",
    "                alpha = 4/(1.0+j+i)+0.0001   \n",
    "                randIndex = int(np.random.uniform(0,len(dataIndex))) \n",
    "                h = expit(sum(X[randIndex]*weights))\n",
    "                error = h-y[randIndex] \n",
    "                weights = weights - alpha * error * X[randIndex]\n",
    "                del (dataIndex[randIndex])\n",
    "        self.coef_=weights[:-1]\n",
    "        self.intercept=weights[-1]\n",
    "        self.weights=np.mat(weights).transpose()\n",
    "        return weights\n",
    "    def predict(self,X):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=(np.concatenate([X,X_],axis=1)) \n",
    "        y_value=np.array(expit(X@self.weights)).flatten()\n",
    "        y_pre=np.array([1 if i>0.5 else 0 for i in y_value])\n",
    "        return y_pre\n",
    "    def predict_prob(self,X):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=(np.concatenate([X,X_],axis=1)) \n",
    "        y_value=np.array(expit(X@self.weights)).flatten()\n",
    "        return y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298245614035088"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "pre=model.predict(X_test)\n",
    "accuracy_score(y_test,pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Scipy求解带l2正则化的LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'ra'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-ae6f294028b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfmin_l_bfgs_b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mra\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'ra'"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self,c=0.0):\n",
    "        self.c=c\n",
    "        self.fitted=False\n",
    "       \n",
    "    def fit(self,X,y):\n",
    "        m,n=X.shape\n",
    "        W=np.random.randn(n+1)\n",
    "        self.res=fmin_l_bfgs_b(type(self).log_loss,x0=W,args=(X,y,self.c))\n",
    "        self.W=self.res[0]\n",
    "        self.coef_=self.W[:-1]\n",
    "        self.intercept_=self.W[-1]\n",
    "        self.fitted=True\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        if self.fitted==False:\n",
    "            raise Exception('The model has not been trained yet, please train the model.')\n",
    "        else:\n",
    "            X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "            X=(np.concatenate([X,X_],axis=1)) \n",
    "            y_prob=expit(X@self.W.T+self.c*np.sum(self.intercept_**2))\n",
    "            return np.array([1 if i>0.5 else 0 for i in y_prob])\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=(np.concatenate([X,X_],axis=1)) \n",
    "        y_prob=expit(X@self.W.T+self.c*np.sum(self.intercept_**2))\n",
    "        return y_prob\n",
    "    \n",
    "    def log_loss(W,X,y,c):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=(np.concatenate([X,X_],axis=1)) \n",
    "        m=X.shape[0]\n",
    "        y_pred=expit(X@W.T+c*np.sum(W[:-1]**2)) \n",
    "        y_pred=np.clip(y_pred,0.0001,0.9999)\n",
    "        loss=np.sum(-(y*np.log(y_pred)+(1-y)*np.log(1-y_pred)))/m\n",
    "        gradient=X.T@(y_pred-y)\n",
    "        return loss,gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: divide by zero encountered in log\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LogisticRegression(c=0).fit(X_train,y_train)\n",
    "lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707602339181286"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.2072749131897207"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
