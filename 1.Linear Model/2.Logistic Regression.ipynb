{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X,y=load_breast_cancer().data,load_breast_cancer().target\n",
    "X=StandardScaler().fit_transform(X)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用sklearn求解Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as sklearn_LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "model=sklearn_LogisticRegression(penalty='none',fit_intercept=True,solver='lbfgs').fit(X_train,y_train)\n",
    "pre=model.predict(X_test)\n",
    "accuracy_score(y_test,pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "class LogisticRegression:\n",
    "    def __init__(self,learning_rate=0.004,max_iters=1800):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.max_iters=max_iters\n",
    "    def fit(self,X,y):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=np.mat(np.concatenate([X,X_],axis=1))       \n",
    "        y = np.mat(y).transpose()\n",
    "        m,n = X.shape\n",
    "        weights = np.mat(np.random.randn(n,1))\n",
    "        for k in range(self.max_iters):\n",
    "            h = expit(X*weights)  \n",
    "            error = h - y            \n",
    "            weights = weights - self.learning_rate * X.transpose()* error\n",
    "        self.coef_=np.array(weights).flatten()[:-1]\n",
    "        self.intercept_=weights[-1]\n",
    "        self.weights=weights\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)  \n",
    "        X=np.concatenate([X,X_],axis=1)\n",
    "        y_value=np.array(expit(X@self.weights)).flatten()                \n",
    "        y_pre=np.array([1 if i>0.5 else 0 for i in y_value])                 \n",
    "        return y_pre\n",
    "                    \n",
    "    def predict_proba(self,X):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=np.concatenate([X,X_],axis=1)   \n",
    "        y_value=np.array(expit(X@self.weights)).flatten()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9532163742690059\n"
     ]
    }
   ],
   "source": [
    "model=LogisticRegression().fit(X_train,y_train)\n",
    "y_pre=model.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用随机梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self,alpha=0.007,maxCycles=1800):\n",
    "        self.alpha=alpha\n",
    "        self.maxCycles=maxCycles\n",
    "    def fit(self,X, y, numIter=150):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=(np.concatenate([X,X_],axis=1)) \n",
    "        m,n = X.shape\n",
    "        weights = np.ones(n)   \n",
    "        for j in range(numIter):\n",
    "            dataIndex = list(range(m))\n",
    "            for i in range(m):\n",
    "                alpha = 4/(1.0+j+i)+0.0001   \n",
    "                randIndex = int(np.random.uniform(0,len(dataIndex))) \n",
    "                h = expit(sum(X[randIndex]*weights))\n",
    "                error = h-y[randIndex] \n",
    "                weights = weights - alpha * error * X[randIndex]\n",
    "                del (dataIndex[randIndex])\n",
    "        self.coef_=weights[:-1]\n",
    "        self.intercept=weights[-1]\n",
    "        self.weights=np.mat(weights).transpose()\n",
    "        return weights\n",
    "    def predict(self,X):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=(np.concatenate([X,X_],axis=1)) \n",
    "        y_value=np.array(expit(X@self.weights)).flatten()\n",
    "        y_pre=np.array([1 if i>0.5 else 0 for i in y_value])\n",
    "        return y_pre\n",
    "    def predict_prob(self,X):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=(np.concatenate([X,X_],axis=1)) \n",
    "        y_value=np.array(expit(X@self.weights)).flatten()\n",
    "        return y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9532163742690059"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "pre=model.predict(X_test)\n",
    "accuracy_score(y_test,pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Scipy求解带l2正则化的LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "class LogisticRegression:\n",
    "    def __init__(self,c=0.0):\n",
    "        self.c=c\n",
    "        self.fitted=False\n",
    "       \n",
    "    def fit(self,X,y):\n",
    "        m,n=X.shape\n",
    "        W=np.random.randn(n+1)\n",
    "        self.res=fmin_l_bfgs_b(type(self).log_loss,x0=W,args=(X,y,self.c))\n",
    "        self.W=self.res[0]\n",
    "        self.coef_=self.W[:-1]\n",
    "        self.intercept_=self.W[-1]\n",
    "        self.fitted=True\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        if self.fitted==False:\n",
    "            raise Exception('The model has not been trained yet, please train the model.')\n",
    "        else:\n",
    "            X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "            X=(np.concatenate([X,X_],axis=1)) \n",
    "            y_prob=expit(X@self.W.T+self.c*np.sum(self.intercept_**2))\n",
    "            return np.array([1 if i>0.5 else 0 for i in y_prob])\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=(np.concatenate([X,X_],axis=1)) \n",
    "        y_prob=expit(X@self.W.T+self.c*np.sum(self.intercept_**2))\n",
    "        return y_prob\n",
    "    \n",
    "    def log_loss(W,X,y,c):\n",
    "        X_=np.full((X.shape[0],1),fill_value=1)   \n",
    "        X=(np.concatenate([X,X_],axis=1)) \n",
    "        m=X.shape[0]\n",
    "        y_pred=expit(X@W.T+c*np.sum(W[:-1]**2))\n",
    "        y_pred=np.clip(y_pred,1e-16,1-1e-16)\n",
    "        loss=np.sum(-(y*np.log(y_pred)+(1-y)*np.log(1-y_pred)))/m\n",
    "        gradient=X.T@(y_pred-y)\n",
    "        return loss,gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LogisticRegression(c=0).fit(X_train,y_train)\n",
    "lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
