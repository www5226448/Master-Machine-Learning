{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "class My_AdaBoostClassifier:\n",
    "    def __init__(self,numIt=5000):\n",
    "        self.numIt=numIt\n",
    "        self.classifers=None\n",
    "              \n",
    "    def fit(self,X,y):\n",
    "        y=2*y-1\n",
    "        self.classifers,_=My_AdaBoostClassifier.adaBoostTrainDS(X,y,self.numIt)\n",
    "        return self\n",
    "        \n",
    "    def predict(self,X):\n",
    "        if self.classifers is None:\n",
    "            raise Exception('The model has not yet been trained.')\n",
    "        return (My_AdaBoostClassifier.adaClassify(X,self.classifers)+1)/2\n",
    "        \n",
    "    def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):\n",
    "        #just classify the data\n",
    "        retArray = ones((shape(dataMatrix)[0],1))\n",
    "        if threshIneq == 'lt':\n",
    "            retArray[dataMatrix[:,dimen] <= threshVal] = -1.0\n",
    "        else:\n",
    "            retArray[dataMatrix[:,dimen] > threshVal] = -1.0\n",
    "        return retArray\n",
    "    \n",
    "    def buildStump(dataArr,classLabels,D):\n",
    "        '''获取dataArr最佳切分列和规则，最小误差，以及切分结果\n",
    "        '''\n",
    "        dataMatrix = mat(dataArr)       #训练数据集X\n",
    "        labelMat = mat(classLabels).T  #训练数据集y\n",
    "        m,n = shape(dataMatrix)   #训练数据集的shape\n",
    "        numSteps = 10.0;      #变量numSteps用于在特征的所有可能值上进行遍历\n",
    "        bestStump = {};     #用于存储给定权重向量D时所得到的最佳单层决策树 的相关信息\n",
    "        bestClasEst = mat(zeros((m,1)))  #初始化最优切分结果\n",
    "        minError = inf #init error sum, to +infinity\n",
    "\n",
    "        for i in range(n):#loop over all dimensions\n",
    "            rangeMin = dataMatrix[:,i].min()\n",
    "            rangeMax = dataMatrix[:,i].max();\n",
    "            stepSize = (rangeMax-rangeMin)/numSteps\n",
    "\n",
    "            for j in range(-1,int(numSteps)+1):#loop over all range in current dimension\n",
    "                for inequal in ['lt', 'gt']: #go over less than and greater than\n",
    "                    threshVal = (rangeMin + float(j) * stepSize)\n",
    "                    predictedVals =My_AdaBoostClassifier.stumpClassify(dataMatrix,i,threshVal,inequal)\n",
    "                    #call stump classify with i, j, lessThan\n",
    "                    errArr = mat(ones((m,1)))\n",
    "                    errArr[predictedVals == labelMat] = 0\n",
    "                    weightedError = D.T*errArr  #calc total error multiplied by D\n",
    "                    #print \"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError)\n",
    "                    if weightedError < minError:\n",
    "                        minError = weightedError\n",
    "                        bestClasEst = predictedVals.copy()\n",
    "                        bestStump['dim'] = i\n",
    "                        bestStump['thresh'] = threshVal\n",
    "                        bestStump['ineq'] = inequal\n",
    "        return bestStump,minError,bestClasEst\n",
    "        \n",
    "    def adaBoostTrainDS(dataArr,classLabels,numIt=500):\n",
    "        weakClassArr = []\n",
    "        m = shape(dataArr)[0]\n",
    "        D = mat(ones((m,1))/m)   #init D to all equal\n",
    "        aggClassEst = mat(zeros((m,1)))\n",
    "        for i in range(numIt):\n",
    "            bestStump,error,classEst = My_AdaBoostClassifier.\\\n",
    "                                        buildStump(dataArr,classLabels,D)#build Stump\n",
    "            alpha = float(0.5*log((1.0-error)/max(error,1e-16)))#calc alpha, throw in max(error,eps) to account for error=0\n",
    "            bestStump['alpha'] = alpha  \n",
    "            weakClassArr.append(bestStump)                  #store Stump Params in Array\n",
    "            expon = multiply(-1*alpha*mat(classLabels).T,classEst) #exponent for D calc, getting messy\n",
    "            D = multiply(D,exp(expon))                              #Calc New D for next iteration\n",
    "            D = D/D.sum()\n",
    "            #calc training error of all classifiers, if this is 0 quit for loop early (use break)\n",
    "            aggClassEst += alpha*classEst\n",
    "            #print \"aggClassEst: \",aggClassEst.T\n",
    "            aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1)))\n",
    "            errorRate = aggErrors.sum()/m\n",
    "            print (\"total error: \",errorRate)\n",
    "            if errorRate == 0.0: \n",
    "                break\n",
    "        return weakClassArr,aggClassEst\n",
    "    \n",
    "    def adaClassify(datToClass,classifierArr):\n",
    "        dataMatrix = mat(datToClass)#do stuff similar to last aggClassEst in adaBoostTrainDS\n",
    "        m = shape(dataMatrix)[0]\n",
    "        aggClassEst = mat(zeros((m,1)))\n",
    "        for i in range(len(classifierArr)):\n",
    "            classEst = My_AdaBoostClassifier.stumpClassify(dataMatrix,classifierArr[i]['dim'],\\\n",
    "                                     classifierArr[i]['thresh'],\\\n",
    "                                     classifierArr[i]['ineq'])#call stump classify\n",
    "            aggClassEst += classifierArr[i]['alpha']*classEst\n",
    "            #print(aggClassEst)\n",
    "        return sign(aggClassEst).flatten().A[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error:  0.08571428571428572\n",
      "total error:  0.08571428571428572\n",
      "total error:  0.02857142857142857\n",
      "total error:  0.07142857142857142\n",
      "total error:  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X,y=make_classification()\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n",
    "model=My_AdaBoostClassifier(numIt=15000).fit(X_train,y_train)\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
