{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return NotImplementedError()\n",
    "\n",
    "    def gradient(self, y, y_pred):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def acc(self, y, y_pred):\n",
    "        return 0\n",
    "\n",
    "class SquareLoss(Loss):\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        return 0.5 * np.power((y - y_pred), 2)\n",
    "\n",
    "    def gradient(self, y, y_pred):\n",
    "        return -(y - y_pred)\n",
    "\n",
    "class CrossEntropy(Loss):\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def loss(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
    "\n",
    "    def acc(self, y, p):\n",
    "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
    "\n",
    "    def gradient(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - (y / p) + (1 - y) / (1 - p)\n",
    "\n",
    "\n",
    "class SotfMaxLoss(Loss):\n",
    "    def gradient(self, y, p):\n",
    "        return y - p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error,accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor as RegressionTree\n",
    "\n",
    "\n",
    "class GBDT(object):\n",
    "    \"\"\"Super class of GradientBoostingClassifier and GradientBoostinRegressor.\n",
    "    Uses a collection of regression trees that trains on predicting the gradient\n",
    "    of the loss function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_estimators: int\n",
    "        树的数量\n",
    "        The number of classification trees that are used.\n",
    "    learning_rate: float\n",
    "        梯度下降的学习率\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    min_samples_split: int\n",
    "        每棵子树的节点的最小数目（小于后不继续切割）\n",
    "        The minimum number of samples needed to make a split when building a tree.\n",
    "    min_impurity: float\n",
    "        每颗子树的最小纯度（小于后不继续切割）\n",
    "        The minimum impurity required to split the tree further.\n",
    "    max_depth: int\n",
    "        每颗子树的最大层数（大于后不继续切割）\n",
    "        The maximum depth of a tree.\n",
    "    regression: boolean\n",
    "        是否为回归问题\n",
    "        True or false depending on if we're doing regression or classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators, learning_rate, min_samples_split,\n",
    "                 min_impurity, max_depth, regression):\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity = min_impurity\n",
    "        self.max_depth = max_depth\n",
    "        self.regression = regression\n",
    "\n",
    "\n",
    "        self.loss = SquareLoss()\n",
    "        if not self.regression:\n",
    "            self.loss = SotfMaxLoss()\n",
    "\n",
    "        # 分类问题也使用回归树，利用残差去学习概率\n",
    "        self.trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            self.trees.append(RegressionTree(min_samples_split=self.min_samples_split,\n",
    "                                             min_impurity_decrease=self.min_impurity,\n",
    "                                             max_depth=self.max_depth))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 让第一棵树去拟合模型\n",
    "        self.trees[0].fit(X, y)\n",
    "        y_pred = self.trees[0].predict(X)\n",
    "        for i in range(1, self.n_estimators):\n",
    "            gradient = self.loss.gradient(y, y_pred)\n",
    "            self.trees[i].fit(X, gradient)\n",
    "            y_pred -= np.multiply(self.learning_rate, self.trees[i].predict(X))\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.trees[0].predict(X)\n",
    "        for i in range(1, self.n_estimators):\n",
    "            y_pred -= np.multiply(self.learning_rate, self.trees[i].predict(X))\n",
    "\n",
    "        if not self.regression:\n",
    "            # Turn into probability distribution\n",
    "    \n",
    "            #print(y_pred)\n",
    "            y_pred = np.exp(y_pred)/np.expand_dims(np.sum(np.exp(y_pred), axis=1),axis=1)\n",
    "            # Set label to the value that maximizes probability\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class GBDTRegressor(GBDT):\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.5, min_samples_split=2,\n",
    "                 min_var_red=1e-7, max_depth=4, debug=False):\n",
    "        super(GBDTRegressor, self).__init__(n_estimators=n_estimators,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            min_samples_split=min_samples_split,\n",
    "                                            min_impurity=min_var_red,\n",
    "                                            max_depth=max_depth,\n",
    "                                            regression=True)\n",
    "\n",
    "\n",
    "class GBDTClassifier(GBDT):\n",
    "    def __init__(self, n_estimators=200, learning_rate=.5, min_samples_split=2,\n",
    "                 min_info_gain=1e-7, max_depth=2, debug=False):\n",
    "        super(GBDTClassifier, self).__init__(n_estimators=n_estimators,\n",
    "                                             learning_rate=learning_rate,\n",
    "                                             min_samples_split=min_samples_split,\n",
    "                                             min_impurity=min_info_gain,\n",
    "                                             max_depth=max_depth,\n",
    "                                             regression=False)\n",
    "        \n",
    "    def to_categorical(x, n_col=None):\n",
    "        \"\"\" One-hot encoding of nominal values \"\"\"\n",
    "        if not n_col:\n",
    "            n_col = np.amax(x) + 1\n",
    "        one_hot = np.zeros((x.shape[0], n_col))\n",
    "        one_hot[np.arange(x.shape[0]), x] = 1\n",
    "        \n",
    "        return one_hot\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y=GBDTClassifier.to_categorical(y)\n",
    "        super(GBDTClassifier, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: RuntimeWarning: overflow encountered in exp\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X,y=make_classification()\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n",
    "model=GBDTClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: RuntimeWarning: overflow encountered in exp\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回归问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 154.37741479,  -48.77815484,   68.03451817,  -35.14246761,\n",
       "        102.71805984, -103.53426139,  -77.55800199,  -71.3650244 ,\n",
       "       -118.69586562,  103.54520733,  -75.92038651,   87.00667933,\n",
       "        -89.15676207,  -30.96593358,  102.28238939, -122.93979502,\n",
       "        206.56501119,  -33.12467489,   14.92415363,   -6.88866231,\n",
       "        -27.70684741,  251.59744608, -221.23354377,  167.03595758,\n",
       "        294.82246583,  187.75188062, -124.31214856,   95.45855911,\n",
       "        -38.0393534 ,  -48.98685183])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X,y=make_regression()\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n",
    "model=GBDTRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141.45886880393275"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(model.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.10853235806722"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model=GradientBoostingRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "model.predict(X_test)\n",
    "mean_absolute_error(model.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
