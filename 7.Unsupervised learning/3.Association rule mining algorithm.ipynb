{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori算法\n",
    "#### 关联规则的三个基本概念\n",
    "* Support\n",
    "* Confidence\n",
    "* Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1, 3, 4}, {2, 3, 5}, {1, 2, 3, 5}, {2, 5}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import *\n",
    " \n",
    "def loadDataSet():\n",
    "    return [{1, 3, 4}, {2, 3, 5}, {1, 2, 3, 5}, {2, 5}]\n",
    "\n",
    "dataSet=loadDataSet()\n",
    "dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4}), frozenset({5})]\n"
     ]
    }
   ],
   "source": [
    "def createC1(dataSet):\n",
    "    '''根据输入的数据集，返回商品集合C1，对C进行去重排序，数据类型为frozenset'''\n",
    "    C1 = []\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction:\n",
    "            if not [item] in C1:\n",
    "                C1.append([item])\n",
    "    C1.sort()\n",
    "    return list(map(frozenset, C1))\n",
    "\n",
    "C1=createC1(dataSet)\n",
    "print(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[frozenset({1}), frozenset({3}), frozenset({2}), frozenset({5})]\n",
      "\n",
      "{frozenset({1}): 0.5, frozenset({3}): 0.75, frozenset({4}): 0.25, frozenset({2}): 0.75, frozenset({5}): 0.75}\n"
     ]
    }
   ],
   "source": [
    "def scanD(D, Ck, minSupport):\n",
    "    '''计算数据集Ck在数据集D中的支持度，并返回大于最小支持度(minSupport)的数据,和所有子项的conf字典\n",
    "    '''\n",
    "    ssCnt = {}      #商品规则集：出现次数\n",
    "    for tid in D:\n",
    "        for can in Ck:\n",
    "            if can.issubset(tid):\n",
    "                ssCnt[can] = ssCnt.get(can,0)+1   #若没有此商品规则集，将其置为1\n",
    "    numItems = len(D)       #原数据集计数\n",
    "    Lk= []              #候选集项Cn生成的频繁项集Lk\n",
    "    supportData = {}    #候选集项Cn的支持度字典\n",
    "    #计算候选项集的支持度, supportData key:候选项， value:支持度\n",
    "    for key in ssCnt:\n",
    "        support = ssCnt[key] / numItems\n",
    "        if support >= minSupport:\n",
    "            Lk.append(key)          #这里返回大于支持度的频繁集\n",
    "        supportData[key] = support   #这里会保存所有后端集的支持度\n",
    "    return Lk,supportData\n",
    "\n",
    "L1,support=scanD(dataSet,C1,minSupport=0.5)\n",
    "print(L1)\n",
    "print()\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[frozenset({1, 3}), frozenset({1, 2}), frozenset({1, 5}), frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5})]\n"
     ]
    }
   ],
   "source": [
    "def aprioriGen(Lk, k):  #根据Lk,生成下一层的可能的频繁集\n",
    "    '''\n",
    "    LK：频繁集列表LK\n",
    "    项集元素个数k\n",
    "    '''\n",
    "    Ck = []\n",
    "    lenLk = len(Lk)\n",
    "    for i in range(lenLk):\n",
    "        L1 = list(Lk[i])[:k - 2]\n",
    "        L1.sort()\n",
    "        for j in range(i + 1, lenLk):    #前k-2个项相同时，将两个集合合并，\n",
    "            L2 = list(Lk[j])[:k - 2]   #可以查看生成图理解为什么是前k-2项\n",
    "            L2.sort()\n",
    "            if L1 == L2:\n",
    "                Ck.append(Lk[i] | Lk[j])\n",
    "    return Ck\n",
    "\n",
    "print(aprioriGen(L1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[frozenset({3}), frozenset({2}), frozenset({5})], [frozenset({2, 5})], []]\n",
      "\n",
      "{frozenset({1}): 0.5, frozenset({3}): 0.75, frozenset({4}): 0.25, frozenset({2}): 0.75, frozenset({5}): 0.75, frozenset({2, 3}): 0.5, frozenset({3, 5}): 0.5, frozenset({2, 5}): 0.75}\n"
     ]
    }
   ],
   "source": [
    "def apriori(dataSet, minSupport = 0.5):\n",
    "    '''根据打她Set，和最小支持度生成可能的频繁集，和所有频繁集对应的速配Pro数据\n",
    "    '''\n",
    "    C1 = createC1(dataSet)\n",
    "    L1, supportData = scanD(dataSet, C1, minSupport)\n",
    "    L = [L1]\n",
    "    k = 2\n",
    "    while (len(L[k-2]) > 0):  #初始时k-2为L的最后一个元素，这里K-2随K进行递增获取列表的最后的一个元素\n",
    "        Lk_1 = L[k-2]           #获取L列表中的最后一组频繁集\n",
    "        Ck = aprioriGen(Lk_1, k)       #生成新的可能的频繁集\n",
    "        Lk, supK = scanD(dataSet, Ck, minSupport)    #返回Ck中的频繁集和所有item-support字典\n",
    "        supportData.update(supK)     #更新此次的item-support到字典\n",
    "        L.append(Lk)               #添加频繁集\n",
    "        k += 1\n",
    "\n",
    "    return L, supportData\n",
    "\n",
    "l1,s1=apriori(dataSet,0.7)\n",
    "print(l1)\n",
    "print()\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcConf(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    '''# 计算是否满足最小可信度,如果满足更新数据'''\n",
    "    prunedH = []\n",
    "    #用每个conseq作为后件\n",
    "    for conseq in H:\n",
    "        # 计算置信度\n",
    "        conf = supportData[freqSet] / supportData[freqSet - conseq]\n",
    "        lift = supportData[freqSet] / (supportData[freqSet - conseq]* supportData[conseq])\n",
    "        if conf >= minConf:\n",
    "            #print(freqSet - conseq, '-->', conseq, 'conf:', conf,'lift',lift)\n",
    "            # 元组中的四个元素：前件、后件、conf,lift\n",
    "            brl.append((freqSet - conseq, conseq, conf,lift)) #更新规则项\n",
    "            prunedH.append(conseq)    \n",
    "    return prunedH   #返回后件列表\n",
    "\n",
    "\n",
    "\n",
    "def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    '''freqSet:频繁集\n",
    "    H：可能成为右件的单个元素\n",
    "    supportData:频繁项-支持度\n",
    "    br1:当前生成的关联规则\n",
    "    minConf：最小置信度\n",
    "    '''\n",
    "    m = len(H[0]) \n",
    "    if (len(freqSet) > (m + 1)):     #查看是否有可能生成大于1的右件，此方法已经假设关联规则的size>=3\n",
    "        Hmp1 = aprioriGen(H, m + 1)   #生成可能的右件项\n",
    "        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)  #计算置信度并更新br1，返回此次新增的右件\n",
    "        if (len(Hmp1) > 0):     #如果此次后件列表存在，继续生成下一层关联规则\n",
    "            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\n",
    "            \n",
    "\n",
    "def generateRules(L, supportData, minConf=0.7):\n",
    "    '''生成关联规则\n",
    "    L: 频繁项集列表\n",
    "    supportData: 包含频繁项集支持数据的字典\n",
    "    minConf:最小置信度\n",
    "    '''\n",
    "    bigRuleList = []  #初始化关联规则列表\n",
    "    for i in range(1, len(L)):  #从频繁集中size=2的频繁项开始遍历\n",
    "        for freqSet in L[i]:\n",
    "            H1 = [frozenset([item]) for item in freqSet]   #频繁集中的频繁项列表\n",
    "            if (i > 1):  #如果频繁集的size大于2\n",
    "                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "            else:  #如果频繁集的size=2\n",
    "                calcConf(freqSet, H1, supportData, bigRuleList, minConf)   \n",
    "    return bigRuleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(frozenset({1}), frozenset({3}), 1.0, 1.3333333333333333),\n",
       " (frozenset({5}), frozenset({2}), 1.0, 1.3333333333333333),\n",
       " (frozenset({2}), frozenset({5}), 1.0, 1.3333333333333333)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=loadDataSet()\n",
    "L,supportData=apriori(data,0.5)\n",
    "generateRules(L,supportData,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(frozenset({3}), frozenset({1}), 0.6666666666666666, 1.3333333333333333),\n",
       " (frozenset({1}), frozenset({3}), 1.0, 1.3333333333333333),\n",
       " (frozenset({3}), frozenset({2}), 0.6666666666666666, 0.8888888888888888),\n",
       " (frozenset({2}), frozenset({3}), 0.6666666666666666, 0.8888888888888888),\n",
       " (frozenset({5}), frozenset({3}), 0.6666666666666666, 0.8888888888888888),\n",
       " (frozenset({3}), frozenset({5}), 0.6666666666666666, 0.8888888888888888),\n",
       " (frozenset({5}), frozenset({2}), 1.0, 1.3333333333333333),\n",
       " (frozenset({2}), frozenset({5}), 1.0, 1.3333333333333333),\n",
       " (frozenset({5}), frozenset({2, 3}), 0.6666666666666666, 1.3333333333333333),\n",
       " (frozenset({3}), frozenset({2, 5}), 0.6666666666666666, 0.8888888888888888),\n",
       " (frozenset({2}), frozenset({3, 5}), 0.6666666666666666, 1.3333333333333333)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateRules(L,supportData,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 挖掘Groceries数据集中的关联规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroceriesData=[]\n",
    "with open('Groceries.csv','r') as f:\n",
    "    for i in range(9835):\n",
    "        line=str(f.readline())\n",
    "        start_index=line.index('{')\n",
    "        end_index=line.index('}')\n",
    "        text=line[start_index+1:end_index]\n",
    "        item=text.split(',')\n",
    "        GroceriesData.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(frozenset({'yogurt'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.40160349854227406,\n",
       "  1.5717351405345263),\n",
       " (frozenset({'butter'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.4972477064220184,\n",
       "  1.9460530014566455),\n",
       " (frozenset({'tropical fruit'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.40310077519379844,\n",
       "  1.5775949558420244),\n",
       " (frozenset({'curd'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.4904580152671756,\n",
       "  1.9194805332879712),\n",
       " (frozenset({'root vegetables'}),\n",
       "  frozenset({'other vegetables'}),\n",
       "  0.43470149253731344,\n",
       "  2.2466049285887952),\n",
       " (frozenset({'hamburger meat'}),\n",
       "  frozenset({'other vegetables'}),\n",
       "  0.41590214067278286,\n",
       "  2.149446954028807),\n",
       " (frozenset({'sugar'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.4444444444444445,\n",
       "  1.7393995666976168),\n",
       " (frozenset({'root vegetables'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.44869402985074625,\n",
       "  1.75603095247994),\n",
       " (frozenset({'whipped/sour cream'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.449645390070922,\n",
       "  1.7597542424781207),\n",
       " (frozenset({'whipped/sour cream'}),\n",
       "  frozenset({'other vegetables'}),\n",
       "  0.40283687943262414,\n",
       "  2.081923651718265),\n",
       " (frozenset({'domestic eggs'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.47275641025641024,\n",
       "  1.8502026640954214),\n",
       " (frozenset({'ham'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.44140625,\n",
       "  1.727509139972145),\n",
       " (frozenset({'frozen vegetables'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.4249471458773784,\n",
       "  1.663093983169127),\n",
       " (frozenset({'beef'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.4050387596899225,\n",
       "  1.5851795469758803),\n",
       " (frozenset({'margarine'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.4131944444444444,\n",
       "  1.6170980346641903),\n",
       " (frozenset({'oil'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.40217391304347827,\n",
       "  1.57396754269105),\n",
       " (frozenset({'butter milk'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.41454545454545455,\n",
       "  1.622385414028868),\n",
       " (frozenset({'chicken'}),\n",
       "  frozenset({'other vegetables'}),\n",
       "  0.4170616113744075,\n",
       "  2.1554392789633727),\n",
       " (frozenset({'hamburger meat'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.4434250764525994,\n",
       "  1.735410118150145),\n",
       " (frozenset({'cream cheese '}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.41538461538461546,\n",
       "  1.6256695950289264),\n",
       " (frozenset({'hard cheese'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.4107883817427386,\n",
       "  1.6076815497174028),\n",
       " (frozenset({'white bread'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.4057971014492754,\n",
       "  1.5881474304630416),\n",
       " (frozenset({'sliced cheese'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.4398340248962655,\n",
       "  1.7213560027277242),\n",
       " (frozenset({'onions'}),\n",
       "  frozenset({'other vegetables'}),\n",
       "  0.45901639344262296,\n",
       "  2.3722681185014167),\n",
       " (frozenset({'chicken'}),\n",
       "  frozenset({'whole milk'}),\n",
       "  0.4099526066350711,\n",
       "  1.6044106192821026)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L,supportData=apriori(GroceriesData,0.01)\n",
    "generateRules(L,supportData,0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP-Tree算法\n",
    "\n",
    "#### 1.创建FP-tree数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pyramid   9\n",
      "   eye   13\n"
     ]
    }
   ],
   "source": [
    "class treeNode:\n",
    "    def __init__(self,nameValue,numOccur,parentNode):\n",
    "        self.name = nameValue   #当前节点的名称，使用商品item进行命名\n",
    "        self.count =numOccur       \n",
    "        self.nodeLink =None   #当前节点的headerTable链表\n",
    "        self.parent =parentNode  #节点的父节点\n",
    "        self.children = {}  #当前节点的子节点\n",
    "        \n",
    "    def inc(self,numOccur):\n",
    "        self.count += numOccur  #发生次数\n",
    "        \n",
    "    def disp(self,ind =1):  #结构化打印树\n",
    "        print(' '*ind,self.name,' ',self.count)\n",
    "        \n",
    "        for child in self.children.values():\n",
    "            child.disp(ind+1)\n",
    "            \n",
    "rootNode=treeNode('pyramid',9,None)\n",
    "rootNode.children['eye']=treeNode('eye',13,None)\n",
    "rootNode.disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pyramid   9\n",
      "   eye   13\n",
      "   phoenix   3\n"
     ]
    }
   ],
   "source": [
    "rootNode.children['phoenix']=treeNode('phoenix',3,None)\n",
    "rootNode.disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSimpDat():\n",
    "    simpDat = [['r', 'z', 'h', 'j', 'p'],\n",
    "               ['z', 'y', 'x', 'w', 'v', 'u', 't', 's'],\n",
    "               ['z'],\n",
    "               ['r', 'x', 'n', 'o', 's'],\n",
    "               ['y', 'r', 'x', 'z', 'q', 't', 'p'],\n",
    "               ['y', 'z', 'x', 'e', 'q', 's', 't', 'm']]\n",
    "    return simpDat\n",
    "\n",
    "def createInitSet(dataSet):      #将原数据的格式更新，便于更新树\n",
    "    retDict = {}\n",
    "    for trans in dataSet:\n",
    "        fset = frozenset(trans)\n",
    "        retDict.setdefault(fset, 0)\n",
    "        retDict[fset] += 1\n",
    "    return retDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  φ   1\n",
      "   z   5\n",
      "    r   1\n",
      "    x   3\n",
      "     y   3\n",
      "      t   3\n",
      "       s   2\n",
      "       r   1\n",
      "   x   1\n",
      "    s   1\n",
      "     r   1\n"
     ]
    }
   ],
   "source": [
    "def createTree(dataSet,minSup=1):\n",
    "    '''根据dataSet遍历两个数据库，创建FP-tree,\n",
    "    第一次，生成headerTable,记录频繁项-频繁项出现的次数\n",
    "    第二次，生成FP-tree，并将headerTable中的item指针指向树中对应的层级节点'''\n",
    "    headerTable={}   #初始化headerTable\n",
    "    for trans in dataSet:  #遍历交易\n",
    "        for item in trans:  #遍历交易中的item\n",
    "            headerTable[item]=headerTable.get(item,0)+ 1   #遍历计算每个item的count\n",
    "            \n",
    "    lessThanMinsup = list(filter(lambda k:headerTable[k] < minSup, headerTable.keys()))\n",
    "    for k in lessThanMinsup: \n",
    "        del(headerTable[k])   #删除不满足最小支持度的item\n",
    "        \n",
    "    freItemSet=set(headerTable.keys())         #获取频繁item的集合\n",
    "    \n",
    "    if len(freItemSet)==0:\n",
    "        return None,None   #如果所有的item都不满足最小支持度，返回None,None\n",
    "    \n",
    "    for k in headerTable.keys():               #格式转化,headerTable的每一项第一个元素为count，\n",
    "        headerTable[k]=[headerTable[k],None]    #第二个元素为指针指向node\n",
    "\n",
    "        \n",
    "    retTree=treeNode('φ', 1, None)     #构建树的根节点，开始第二次遍历数据集，构建FP-tree\n",
    "    for tranSet,count in dataSet.items():   \n",
    "        localD={}               #初始化这条交易的item,和此item出现的次数\n",
    "        for item in tranSet:\n",
    "            if item in freItemSet:\n",
    "                localD[item]=headerTable[item][0]     #获取对应item出现的次数，下一步排序时需要\n",
    "                \n",
    "        if len(localD)>0:     #如果此条tran中的频繁项大于0，进行排序\n",
    "            orderItems=[v[0] for v in \n",
    "            sorted(localD.items(),key=lambda p:(p[1],p[0]),reverse=True)] #将此条trans的item排序\n",
    "            updateTree(\n",
    "                orderItems,retTree,headerTable,count)  #更新树,orderItems为排序后的trans,count为相同的trans出现次数\n",
    "       \n",
    "    return retTree,headerTable\n",
    "    \n",
    "def updateTree(items, inTree, headerTable, count):\n",
    "\n",
    "    if items[0] in inTree.children:  # 检查items的第一个元素是否位于根节点之下\n",
    "        inTree.children[items[0]].inc(count)  # 更新树中item的count\n",
    "    else:  # 如果不位于根节点之下\n",
    "        inTree.children[items[0]] = treeNode(items[0], count, inTree)  #将其添加在根节点下\n",
    "        if headerTable[items[0]][1] == None:  # 如果headerTable之前不存在指针，添加指针\n",
    "            headerTable[items[0]][1] = inTree.children[items[0]]\n",
    "        else:  #如果headerTable存在指针，使用更新header方法连接新节点\n",
    "            updateHeader(headerTable[items[0]][1], inTree.children[items[0]])\n",
    "\n",
    "    if len(items) > 1:  # 如果items的size大于1，迭代调用\n",
    "        updateTree(items[1:], inTree.children[items[0]], headerTable, count)\n",
    "\n",
    "\n",
    "def updateHeader(nodeToTest, targetNode): \n",
    "    '''如果headertable的指针已经存在，不断迭代，直至\n",
    "    找到None节点\n",
    "    '''\n",
    "    while (nodeToTest.nodeLink != None):  \n",
    "        nodeToTest = nodeToTest.nodeLink  #获取下下一级目标，直到目标节点为None\n",
    "    nodeToTest.nodeLink = targetNode\n",
    "    \n",
    "    \n",
    "simpDat = loadSimpDat()\n",
    "dictDat = createInitSet(simpDat)\n",
    "myFPTree,myheader = createTree(dictDat, 3)\n",
    "myFPTree.disp()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.从FP-tree中挖掘频繁项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascendTree(leafNode, prefixPath):\n",
    "    if leafNode.parent != None:\n",
    "        prefixPath.append(leafNode.name)\n",
    "        ascendTree(leafNode.parent, prefixPath)\n",
    "\n",
    "def findPrefixPath(basePat, headTable):\n",
    "    condPats = {}\n",
    "    treeNode = headTable[basePat][1]\n",
    "    while treeNode != None:\n",
    "        prefixPath = []\n",
    "        ascendTree(treeNode, prefixPath)\n",
    "        if len(prefixPath) > 1:\n",
    "            condPats[frozenset(prefixPath[1:])] = treeNode.count\n",
    "        treeNode = treeNode.nodeLink\n",
    "    return condPats\n",
    "\n",
    "def mineTree(inTree, headerTable, minSup=1, preFix=set([]), freqItemList=[]):\n",
    "    # order by minSup asc, value asc\n",
    "    bigL = [v[0] for v in sorted(headerTable.items(), key=lambda p: (p[1][0],p[0]))]\n",
    "    for basePat in bigL:\n",
    "        newFreqSet = preFix.copy()\n",
    "        newFreqSet.add(basePat)\n",
    "        freqItemList.append(newFreqSet)\n",
    "        # 通过条件模式基找到的频繁项集\n",
    "        condPattBases = findPrefixPath(basePat, headerTable)\n",
    "        myCondTree, myHead = createTree(condPattBases, minSup)\n",
    "        if myHead != None:\n",
    "            print('condPattBases: ', basePat, condPattBases)\n",
    "            myCondTree.disp()\n",
    "            print('*' * 30)\n",
    "\n",
    "            mineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  φ   1\n",
      "   z   5\n",
      "    r   1\n",
      "    x   3\n",
      "     y   3\n",
      "      t   3\n",
      "       s   2\n",
      "       r   1\n",
      "   x   1\n",
      "    s   1\n",
      "     r   1\n",
      "z {}\n",
      "x {frozenset({'z'}): 3}\n",
      "y {frozenset({'z', 'x'}): 3}\n",
      "t {frozenset({'y', 'z', 'x'}): 3}\n",
      "s {frozenset({'y', 't', 'z', 'x'}): 2, frozenset({'x'}): 1}\n",
      "r {frozenset({'z'}): 1, frozenset({'s', 'x'}): 1, frozenset({'y', 't', 'z', 'x'}): 1}\n",
      "condPattBases:  r {frozenset({'z'}): 1, frozenset({'s', 'x'}): 1, frozenset({'y', 't', 'z', 'x'}): 1}\n",
      "  φ   1\n",
      "   z   2\n",
      "    x   1\n",
      "   x   1\n",
      "******************************\n",
      "condPattBases:  s {frozenset({'y', 't', 'z', 'x'}): 2, frozenset({'x'}): 1}\n",
      "  φ   1\n",
      "   x   3\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "simpDat = loadSimpDat()\n",
    "dictDat = createInitSet(simpDat)\n",
    "myFPTree,myheader = createTree(dictDat, 3)\n",
    "myFPTree.disp()\n",
    "condPats = findPrefixPath('z', myheader)\n",
    "print('z', condPats)\n",
    "condPats = findPrefixPath('x', myheader)\n",
    "print('x', condPats)\n",
    "condPats = findPrefixPath('y', myheader)\n",
    "print('y', condPats)\n",
    "condPats = findPrefixPath('t', myheader)\n",
    "print('t', condPats)\n",
    "condPats = findPrefixPath('s', myheader)\n",
    "print('s', condPats)\n",
    "condPats = findPrefixPath('r', myheader)\n",
    "print('r', condPats)\n",
    "\n",
    "mineTree(myFPTree, myheader, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
