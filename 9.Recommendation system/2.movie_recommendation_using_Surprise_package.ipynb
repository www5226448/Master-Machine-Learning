{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://osloyi5le.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%B8%88banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python推荐系统库Surprise\n",
    "by [@寒小阳](http://blog.csdn.net/han_xiaoyang)\n",
    "\n",
    "本教程讲解了基于scikit系列库scikit-surprise进行推荐的方法。包含近邻算法(协同过滤)，基于矩阵分解的方法(隐语义模型，NMF)等算法的推荐应用。\n",
    "\n",
    "涉及的数据包含movielens数据集，与爬取的Twitter影评数据集(持续更新)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Surprise.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在推荐系统的建模过程中，我们将用到python库 [Surprise(Simple Python RecommendatIon System Engine)](https://github.com/NicolasHug/Surprise)，是scikit系列中的一个(很多同学用过scikit-learn和scikit-image等库)。\n",
    "\n",
    "### 简单易用，同时支持多种推荐算法：\n",
    "* [基础算法/baseline algorithms](http://surprise.readthedocs.io/en/stable/basic_algorithms.html)\n",
    "* [基于近邻方法(协同过滤)/neighborhood methods](http://surprise.readthedocs.io/en/stable/knn_inspired.html)\n",
    "* [矩阵分解方法/matrix factorization-based (SVD, PMF, SVD++, NMF)](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD)\n",
    "\n",
    "| 算法类名        | 说明  |\n",
    "| ------------- |:-----|\n",
    "|[random_pred.NormalPredictor](http://surprise.readthedocs.io/en/stable/basic_algorithms.html#surprise.prediction_algorithms.random_pred.NormalPredictor)|Algorithm predicting a random rating based on the distribution of the training set, which is assumed to be normal.|\n",
    "|[baseline_only.BaselineOnly](http://surprise.readthedocs.io/en/stable/basic_algorithms.html#surprise.prediction_algorithms.baseline_only.BaselineOnly)|Algorithm predicting the baseline estimate for given user and item.|\n",
    "|[knns.KNNBasic](http://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBasic)|A basic collaborative filtering algorithm.|\n",
    "|[knns.KNNWithMeans](http://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNWithMeans)|A basic collaborative filtering algorithm, taking into account the mean ratings of each user.|\n",
    "|[knns.KNNBaseline](http://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBaseline)|A basic collaborative filtering algorithm taking into account a baseline rating.|\t\n",
    "|[matrix_factorization.SVD](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD)|The famous SVD algorithm, as popularized by Simon Funk during the Netflix Prize.|\n",
    "|[matrix_factorization.SVDpp](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp)|The SVD++ algorithm, an extension of SVD taking into account implicit ratings.|\n",
    "|[matrix_factorization.NMF](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.NMF)|A collaborative filtering algorithm based on Non-negative Matrix Factorization.|\n",
    "|[slope_one.SlopeOne](http://surprise.readthedocs.io/en/stable/slope_one.html#surprise.prediction_algorithms.slope_one.SlopeOne)|A simple yet accurate collaborative filtering algorithm.|\n",
    "|[co_clustering.CoClustering](http://surprise.readthedocs.io/en/stable/co_clustering.html#surprise.prediction_algorithms.co_clustering.CoClustering)|A collaborative filtering algorithm based on co-clustering.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其中基于近邻的方法(协同过滤)可以设定不同的度量准则。\n",
    "\n",
    "| 相似度度量标准 | 度量标准说明  |\n",
    "| ------------- |:-----|\n",
    "|[cosine](http://surprise.readthedocs.io/en/stable/similarities.html#surprise.similarities.cosine)|Compute the cosine similarity between all pairs of users (or items).|\n",
    "|[msd](http://surprise.readthedocs.io/en/stable/similarities.html#surprise.similarities.msd)|Compute the Mean Squared Difference similarity between all pairs of users (or items).|\n",
    "|[pearson](http://surprise.readthedocs.io/en/stable/similarities.html#surprise.similarities.pearson)|Compute the Pearson correlation coefficient between all pairs of users (or items).|\n",
    "|[pearson_baseline](http://surprise.readthedocs.io/en/stable/similarities.html#surprise.similarities.pearson_baseline)|Compute the (shrunk) Pearson correlation coefficient between all pairs of users (or items) using baselines for centering instead of means.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持不同的评估准则\n",
    "| 评估准则 | 准则说明  |\n",
    "| ------------- |:-----|\n",
    "|[rmse](http://surprise.readthedocs.io/en/stable/accuracy.html#surprise.accuracy.rmse)|Compute RMSE (Root Mean Squared Error).|\n",
    "|[msd](http://surprise.readthedocs.io/en/stable/similarities.html#surprise.similarities.msd)|Compute MAE (Mean Absolute Error).|\n",
    "|[fcp](http://surprise.readthedocs.io/en/stable/accuracy.html#surprise.accuracy.fcp)|Compute FCP (Fraction of Concordant Pairs).|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本使用方法如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 可以使用上面提到的各种推荐系统算法\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import evaluate, print_perf\n",
    "\n",
    "# 默认载入movielens数据集\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "# k折交叉验证(k=3)\n",
    "data.split(n_folds=3)\n",
    "# 试一把SVD矩阵分解\n",
    "algo = SVD()\n",
    "# 在数据集上测试一下效果\n",
    "perf = evaluate(algo, data, measures=['RMSE', 'MAE'])\n",
    "#输出结果\n",
    "print_perf(perf)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入自己的数据集方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 指定文件所在路径\n",
    "file_path = os.path.expanduser('~/.surprise_data/ml-100k/ml-100k/u.data')\n",
    "# 告诉文本阅读器，文本的格式是怎么样的\n",
    "reader = Reader(line_format='user item rating timestamp', sep='\\t')\n",
    "# 加载数据\n",
    "data = Dataset.load_from_file(file_path, reader=reader)\n",
    "# 手动切分成5折(方便交叉验证)\n",
    "data.split(n_folds=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 算法调参(让推荐系统有更好的效果)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里实现的算法用到的算法无外乎也是SGD等，因此也有一些超参数会影响最后的结果，我们同样可以用sklearn中常用到的网格搜索交叉验证(GridSearchCV)来选择最优的参数。简单的例子如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 定义好需要优选的参数网格\n",
    "param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n",
    "              'reg_all': [0.4, 0.6]}\n",
    "# 使用网格搜索交叉验证\n",
    "grid_search = GridSearch(SVD, param_grid, measures=['RMSE', 'FCP'])\n",
    "# 在数据集上找到最好的参数\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "data.split(n_folds=3)\n",
    "grid_search.evaluate(data)\n",
    "# 输出调优的参数组 \n",
    "# 输出最好的RMSE结果\n",
    "print(grid_search.best_score['RMSE'])\n",
    "# >>> 0.96117566386\n",
    "\n",
    "# 输出对应最好的RMSE结果的参数\n",
    "print(grid_search.best_params['RMSE'])\n",
    "# >>> {'reg_all': 0.4, 'lr_all': 0.005, 'n_epochs': 10}\n",
    "\n",
    "# 最好的FCP得分\n",
    "print(grid_search.best_score['FCP'])\n",
    "# >>> 0.702279736531\n",
    "\n",
    "# 对应最高FCP得分的参数\n",
    "print(grid_search.best_params['FCP'])\n",
    "# >>> {'reg_all': 0.6, 'lr_all': 0.005, 'n_epochs': 10}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在我们的数据集上训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建模和存储模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.用协同过滤构建模型并进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 movielens的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/surprise/evaluate.py:66: UserWarning: The evaluate() method is deprecated. Please use model_selection.cross_validate() instead.\n",
      "  'model_selection.cross_validate() instead.', UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/surprise/dataset.py:193: UserWarning: Using data.split() or using load_from_folds() without using a CV iterator is now deprecated. \n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "RMSE: 0.9390\n",
      "MAE:  0.7398\n",
      "------------\n",
      "Fold 2\n",
      "RMSE: 0.9551\n",
      "MAE:  0.7548\n",
      "------------\n",
      "Fold 3\n",
      "RMSE: 0.9451\n",
      "MAE:  0.7456\n",
      "------------\n",
      "------------\n",
      "Mean RMSE: 0.9464\n",
      "Mean MAE : 0.7467\n",
      "------------\n",
      "------------\n",
      "        Fold 1  Fold 2  Fold 3  Mean    \n",
      "RMSE    0.9390  0.9551  0.9451  0.9464  \n",
      "MAE     0.7398  0.7548  0.7456  0.7467  \n"
     ]
    }
   ],
   "source": [
    "# 可以使用上面提到的各种推荐系统算法\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import evaluate, print_perf\n",
    "\n",
    "# 默认载入movielens数据集\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "# k折交叉验证(k=3)\n",
    "data.split(n_folds=3)\n",
    "# 试一把SVD矩阵分解\n",
    "algo = SVD()\n",
    "# 在数据集上测试一下效果\n",
    "perf = evaluate(algo, data, measures=['RMSE', 'MAE'])\n",
    "#输出结果\n",
    "print_perf(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/surprise/prediction_algorithms/algo_base.py:51: UserWarning: train() is deprecated. Use fit() instead\n",
      "  warnings.warn('train() is deprecated. Use fit() instead', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      "The 10 nearest neighbors of Toy Story are:\n",
      "Beauty and the Beast (1991)\n",
      "Raiders of the Lost Ark (1981)\n",
      "That Thing You Do! (1996)\n",
      "Lion King, The (1994)\n",
      "Craft, The (1996)\n",
      "Liar Liar (1997)\n",
      "Aladdin (1992)\n",
      "Cool Hand Luke (1967)\n",
      "Winnie the Pooh and the Blustery Day (1968)\n",
      "Indiana Jones and the Last Crusade (1989)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "以下的程序段告诉大家如何在协同过滤算法建模以后，根据一个item取回相似度最高的item，主要是用到algo.get_neighbors()这个函数\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "import os\n",
    "import io\n",
    "\n",
    "from surprise import KNNBaseline\n",
    "from surprise import Dataset\n",
    "\n",
    "\n",
    "def read_item_names():\n",
    "    \"\"\"\n",
    "    获取电影名到电影id 和 电影id到电影名的映射\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = (os.path.expanduser('~') +\n",
    "                 '/.surprise_data/ml-100k/ml-100k/u.item')\n",
    "    rid_to_name = {}\n",
    "    name_to_rid = {}\n",
    "    with io.open(file_name, 'r', encoding='ISO-8859-1') as f:\n",
    "        for line in f:\n",
    "            line = line.split('|')\n",
    "            rid_to_name[line[0]] = line[1]\n",
    "            name_to_rid[line[1]] = line[0]\n",
    "\n",
    "    return rid_to_name, name_to_rid\n",
    "\n",
    "\n",
    "# 首先，用算法计算相互间的相似度\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset = data.build_full_trainset()\n",
    "sim_options = {'name': 'pearson_baseline', 'user_based': False}\n",
    "algo = KNNBaseline(sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "\n",
    "# 获取电影名到电影id 和 电影id到电影名的映射\n",
    "rid_to_name, name_to_rid = read_item_names()\n",
    "\n",
    "# 拿出来Toy Story这部电影对应的item id\n",
    "toy_story_raw_id = name_to_rid['Toy Story (1995)']\n",
    "toy_story_inner_id = algo.trainset.to_inner_iid(toy_story_raw_id)\n",
    "\n",
    "# 找到最近的10个邻居\n",
    "toy_story_neighbors = algo.get_neighbors(toy_story_inner_id, k=10)\n",
    "\n",
    "# 从近邻的id映射回电影名称\n",
    "toy_story_neighbors = (algo.trainset.to_raw_iid(inner_id)\n",
    "                       for inner_id in toy_story_neighbors)\n",
    "toy_story_neighbors = (rid_to_name[rid]\n",
    "                       for rid in toy_story_neighbors)\n",
    "\n",
    "print()\n",
    "print('The 10 nearest neighbors of Toy Story are:')\n",
    "for movie in toy_story_neighbors:\n",
    "    print(movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Twitter电影预测的例子\n",
    "我们会用到Twitter上的电影打分内容提取的user和rating信息，结合协同过滤，做相关推荐。最新的Twitter movie rating数据可以在[MovieTweetings](https://github.com/sidooms/MovieTweetings)上取到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process data and build dataset...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
    "import os\n",
    "import io\n",
    "\n",
    "from surprise import KNNBaseline, Reader\n",
    "from surprise import Dataset\n",
    "\n",
    "file_path = os.path.expanduser('./ratings.dat')\n",
    "# 指定文件格式\n",
    "reader = Reader(line_format='user item rating timestamp', sep='::')\n",
    "# 从文件读取数据\n",
    "movie_data = Dataset.load_from_file(file_path, reader=reader)\n",
    "# 计算电影和电影之间的相似度\n",
    "print(\"process data and build dataset...\")\n",
    "trainset = movie_data.build_full_trainset()\n",
    "sim_options = {'name': 'pearson_baseline', 'user_based': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28682"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50201"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/surprise/prediction_algorithms/algo_base.py:51: UserWarning: train() is deprecated. Use fit() instead\n",
      "  warnings.warn('train() is deprecated. Use fit() instead', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      "The 10 nearest neighbors of <Pirates of the Caribbean: Dead Men Tell No Tales (2017)> are:\n",
      "X-Men: Apocalypse (2016)\n",
      "The Fate of the Furious (2017)\n",
      "Deadpool (2016)\n",
      "Thor: The Dark World (2013)\n",
      "The Expendables 3 (2014)\n",
      "Passengers (2016)\n",
      "Transcendence (2014)\n",
      "Deepwater Horizon (2016)\n",
      "Night at the Museum: Secret of the Tomb (2014)\n",
      "Jack Reacher: Never Go Back (2016)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "以下的程序段告诉大家如何在协同过滤算法建模以后，根据一个item取回相似度最高的item，主要是用到algo.get_neighbors()这个函数\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "import os\n",
    "import io\n",
    "\n",
    "from surprise import KNNBaseline\n",
    "from surprise import Dataset\n",
    "\n",
    "\n",
    "def read_item_names():\n",
    "    \"\"\"\n",
    "    获取电影名到电影id 和 电影id到电影名的映射\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = (os.path.expanduser('./movies.dat'))\n",
    "    rid_to_name = {}\n",
    "    name_to_rid = {}\n",
    "    with io.open(file_name, 'r', encoding='ISO-8859-1') as f:\n",
    "        for line in f:\n",
    "            line = line.split('::')\n",
    "            rid_to_name[line[0]] = line[1]\n",
    "            name_to_rid[line[1]] = line[0]\n",
    "\n",
    "    return rid_to_name, name_to_rid\n",
    "\n",
    "\n",
    "# 首先，用算法计算相互间的相似度\n",
    "algo = KNNBaseline(sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "\n",
    "# 获取电影名到电影id 和 电影id到电影名的映射\n",
    "rid_to_name, name_to_rid = read_item_names()\n",
    "\n",
    "# 拿出来Pirates of the Caribbean: Dead Men Tell No Tales这部电影对应的item id\n",
    "movie_raw_id = name_to_rid[\"Pirates of the Caribbean: Dead Men Tell No Tales (2017)\"]\n",
    "movie_inner_id = algo.trainset.to_inner_iid(movie_raw_id)\n",
    "\n",
    "# 找到最近的10个邻居\n",
    "movie_neighbors = algo.get_neighbors(movie_inner_id, k=10)\n",
    "\n",
    "# 从近邻的id映射回电影名称\n",
    "movie_neighbors = (algo.trainset.to_raw_iid(inner_id)\n",
    "                       for inner_id in movie_neighbors)\n",
    "movie_neighbors = (rid_to_name[rid]\n",
    "                       for rid in movie_neighbors)\n",
    "\n",
    "print()\n",
    "print(\"The 10 nearest neighbors of <Pirates of the Caribbean: Dead Men Tell No Tales (2017)> are:\")\n",
    "for movie in movie_neighbors:\n",
    "    print(movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 对用户进行推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 1000       item: 369        r_ui = 5.00   est = 5.00   {'was_impossible': False} It's a Wonderful Life (1946)\n",
      "user: 1000       item: 1778       r_ui = 5.00   est = 5.00   {'was_impossible': False} The Thinning (2016)\n",
      "user: 1000       item: 3389       r_ui = 5.00   est = 5.00   {'was_impossible': False} Wasati (2016)\n"
     ]
    }
   ],
   "source": [
    "user_inner_id = 1000\n",
    "user_rating = trainset.ur[user_inner_id]\n",
    "items = map(lambda x:x[0], user_rating)\n",
    "for movie in items:\n",
    "    print(algo.predict(user_inner_id, movie, r_ui=5), rid_to_name[algo.trainset.to_raw_iid(movie)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.用矩阵分解进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/surprise/prediction_algorithms/algo_base.py:51: UserWarning: train() is deprecated. Use fit() instead\n",
      "  warnings.warn('train() is deprecated. Use fit() instead', UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-93fc013facd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVDpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#trainset = movie.build_full_trainset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/surprise/prediction_algorithms/algo_base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainset)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/surprise/prediction_algorithms/matrix_factorization.pyx\u001b[0m in \u001b[0;36msurprise.prediction_algorithms.matrix_factorization.SVDpp.fit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/surprise/prediction_algorithms/matrix_factorization.pyx\u001b[0m in \u001b[0;36msurprise.prediction_algorithms.matrix_factorization.SVDpp.sgd\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/surprise/trainset.py\u001b[0m in \u001b[0;36mall_ratings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_ratings\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mu_ratings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_testset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### 使用SVDpp\n",
    "from surprise import SVDpp, evaluate\n",
    "from surprise import Dataset\n",
    "\n",
    "#file_path = os.path.expanduser('./ratings.dat')\n",
    "# 指定文件格式\n",
    "#reader = Reader(line_format='user item rating timestamp', sep='::')\n",
    "# 从文件读取数据\n",
    "#movie_data = Dataset.load_from_file(file_path, reader=reader)\n",
    "# 构建数据集和建模\n",
    "algo = SVDpp()\n",
    "#trainset = movie.build_full_trainset()\n",
    "algo.train(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inner_id = 1000\n",
    "user_rating = trainset.ur[user_inner_id]\n",
    "items = map(lambda x:x[0], user_rating)\n",
    "for movie in items:\n",
    "    print(algo.predict(algo.trainset.to_raw_uid(user_inner_id), algo.trainset.to_raw_iid(movie), r_ui=5), algo.trainset.to_raw_iid(movie))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.模型存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import surprise\n",
    "surprise.dump.dump('./recommendation.model', algo=algo)\n",
    "# 可以用下面的方式载入\n",
    "algo = surprise.dump.load('./recommendation.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.不同的推荐系统算法评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from surprise import Reader, Dataset\n",
    "\n",
    "file_path = os.path.expanduser('./ratings.dat')\n",
    "# 指定文件格式\n",
    "reader = Reader(line_format='user item rating timestamp', sep='::')\n",
    "# 从文件读取数据\n",
    "#movie_data = Dataset.load_from_file(file_path, reader=reader)\n",
    "# 构建数据集\n",
    "trainset = movie.build_full_trainset()\n",
    "# 分成5折\n",
    "trainset.split(n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset.raw_ratings[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 使用NormalPredictor\n",
    "from surprise import NormalPredictor, evaluate\n",
    "algo = NormalPredictor()\n",
    "perf = evaluate(algo, trainset, measures=['RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 使用BaselineOnly\n",
    "from surprise import BaselineOnly, evaluate\n",
    "algo = BaselineOnly()\n",
    "perf = evaluate(algo, trainset, measures=['RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 使用基础版协同过滤\n",
    "from surprise import KNNBasic, evaluate\n",
    "algo = KNNBasic()\n",
    "perf = evaluate(algo, trainset, measures=['RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 使用均值协同过滤\n",
    "from surprise import KNNWithMeans, evaluate\n",
    "algo = KNNWithMeans()\n",
    "perf = evaluate(algo, trainset, measures=['RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 使用协同过滤baseline\n",
    "from surprise import KNNBaseline, evaluate\n",
    "algo = KNNBaseline()\n",
    "perf = evaluate(algo, trainset, measures=['RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 使用SVD\n",
    "from surprise import SVD, evaluate\n",
    "algo = SVD()\n",
    "perf = evaluate(algo, trainset, measures=['RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 使用SVD++\n",
    "from surprise import SVDpp, evaluate\n",
    "algo = SVDpp()\n",
    "perf = evaluate(algo, trainset, measures=['RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 使用NMF\n",
    "from surprise import NMF\n",
    "algo = NMF()\n",
    "perf = evaluate(algo, trainset, measures=['RMSE', 'MAE'])\n",
    "print_perf(perf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
